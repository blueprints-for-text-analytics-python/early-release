{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "# Remark<div class='tocSkip'/>\n",
                "\n",
                "The code in this notebook differs slightly from the printed book, because we removed some boilerplate parts of it. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
                "\n",
                "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book. Numbers in the book may have less decimal places as shown here in the notebook. We also used `textwrap` to control linebreaks for the book. The respective statements here are also just for formatting - you can ignore them.\n",
                "\n",
                "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
                "\n",
                "All of this is done to simplify the code in the book and put the focus on the important parts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "a3UVlFeb3-j1",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "# Setup<div class='tocSkip'/>\n",
                "\n",
                "## Determine Environment<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 52
                },
                "colab_type": "code",
                "id": "Z5CvQu904xx4",
                "outputId": "445c085e-16ea-4515-c059-1cb9edc3f374",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "import sys\n",
                "ON_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if ON_COLAB:\n",
                "    BASE_DIR = \"/content\"\n",
                "    print(\"You are working on Google Colab.\")\n",
                "    print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
                "    # adjust release\n",
                "    GIT_ROOT = \"https://github.com/blueprints-for-text-analytics-python/early-release/raw/master\"\n",
                "else:\n",
                "    BASE_DIR = \"..\"\n",
                "    print(\"You are working on a local system.\")\n",
                "    print(f'Files will be searched relative to \"{BASE_DIR}\".')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Download data files<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "import os, subprocess\n",
                "from subprocess import PIPE\n",
                "\n",
                "required_files = [\n",
                "                  'settings.py',\n",
                "                  'packages/blueprints/__init__.py',\n",
                "                  'packages/blueprints/exploration.py',\n",
                "                  'data/reddit-selfposts/reddit-selfposts.db.gz',\n",
                "                  'ch04/colab_requirements.txt'\n",
                "]\n",
                "\n",
                "if ON_COLAB:\n",
                "    print(\"Downloading required files ...\")\n",
                "    for file in required_files:\n",
                "        cmd = ['wget', '-P', os.path.dirname(BASE_DIR+'/'+file), GIT_ROOT+'/'+file]\n",
                "        print('!'+' '.join(cmd))\n",
                "        stdout, stderr = subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE).communicate()\n",
                "        # print(stderr.decode()) # uncomment in case of problems"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "XR2aPCyiCAJL",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Install required libraries and additional setup<div class='tocSkip'/>\n",
                "\n",
                "It may take a moment to install the required Python libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "if ON_COLAB:\n",
                "    print(\"\\nAdditional setup ...\")\n",
                "    setup_cmds = ['pip install -r ch13/colab_requirements.txt',\n",
                "                  'mkdir -p models',\n",
                "                 # f'gunzip -k {BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db.gz'\n",
                "                 ]\n",
                "\n",
                "    for cmd in setup_cmds:\n",
                "        print('!'+cmd)\n",
                "        if os.system(cmd) != 0:\n",
                "            print('  --> ERROR')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Common Imports<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'png'\n",
                "\n",
                "%run \"$BASE_DIR/settings.py\"\n",
                "\n",
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "InteractiveShell.ast_node_interactivity = \"all\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# to import blueprints package\n",
                "import os, sys\n",
                "sys.path.append(BASE_DIR + '/packages')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
                "pd.set_option('display.html.use_mathjax', False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# How to Prepare Textual Data For Statistics and Machine Learning\n",
                "## What you'll learn and what we build\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# A Data Preprocessing Pipeline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Introducing the Data Set: Reddit Self Posts\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Loading Data into Pandas\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "posts_file = \"../data/reddit-selfposts/rspct.tsv.gz\"\n",
                "posts_file = \"../data/reddit-selfposts/rspct_autos.tsv.gz\" ### for faster loads use this subset\n",
                "posts_df = pd.read_csv(posts_file, sep='\\t')\n",
                "\n",
                "subred_file = \"../data/reddit-selfposts/subreddit_info.csv.gz\"\n",
                "subred_df = pd.read_csv(subred_file).set_index(['subreddit'])\n",
                "\n",
                "df = posts_df.join(subred_df, on='subreddit')\n",
                "len(df) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# write subset with autos to rspct_autos.tsv.gz\n",
                "\n",
                "# auto_subreddits = subred_df[subred_df['category_1'] == 'autos'].index.to_list()\n",
                "# posts_df[posts_df.subreddit.isin(auto_subreddits)] \\\n",
                "#   .to_csv('../data/reddit-selfposts/rspct_autos.tsv.gz', sep='\\t', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Standardizing Attribute Names\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "print(df.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "column_mapping = {\n",
                "    'id': 'id',\n",
                "    'subreddit': 'subreddit',\n",
                "    'title': 'title',\n",
                "    'selftext': 'text',\n",
                "    'category_1': 'category',\n",
                "    'category_2': 'subcategory',  \n",
                "    'category_3': None, # no data\n",
                "    'in_data': None, # not needed\n",
                "    'reason_for_exclusion': None # not needed\n",
                "}\n",
                "\n",
                "# define remaining columns\n",
                "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
                "\n",
                "# select and rename those columns\n",
                "df = df[columns].rename(columns=column_mapping)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df = df[df['category'] == 'autos']\n",
                "len(df) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "pd.options.display.max_colwidth = None ###\n",
                "df.sample(1, random_state=7).T\n",
                "pd.options.display.max_colwidth = 200 ###"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Checking for Missing Values\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df.isna().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Saving and Loading a Data Frame\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df.to_pickle(\"reddit_dataframe.pkl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import sqlite3\n",
                "\n",
                "db_path = \"../data/reddit-selfposts/reddit-selfposts.db\"\n",
                "\n",
                "con = sqlite3.connect(db_path)\n",
                "df.to_sql(\"posts\", con, index=False, if_exists=\"replace\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "db_path = \"../data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df = pd.read_sql(\"select * from posts\", con)\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "len(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Cleaning Textual Data with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "text = \"\"\"\n",
                "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
                "it got me thinking about the best match ups.\n",
                "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
                "Captain America<lb>\"\"\"\n",
                "\n",
                "text = text.replace('\\n', ' ').strip() ###\n",
                "print(text) ###"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Blueprint: Identifying Dirty Data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
                "\n",
                "def impurity(text, min_len=10):\n",
                "    if text == None or len(text) < min_len:\n",
                "        return 0\n",
                "    else:\n",
                "        # return share of suspicious characters in a text\n",
                "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
                "\n",
                "print(impurity(text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "pd.options.display.max_colwidth = 100 ###\n",
                "# add new column to data frame\n",
                "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
                "\n",
                "# get the top 3 records\n",
                "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)\n",
                "pd.options.display.max_colwidth = 200 ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from blueprints.exploration import count_words ###\n",
                "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Blueprint: Text-Cleaning with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import html\n",
                "\n",
                "def clean(text):\n",
                "    # convert html escapes like &amp; to characters.\n",
                "    text = html.unescape(text) \n",
                "    # tags like <tab>\n",
                "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
                "    # markdown URLs like [Some text](https://....)\n",
                "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
                "    # text or code in brackets like [0]\n",
                "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
                "    # standalone sequences of specials, matches &# but not #cool\n",
                "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
                "    # standalone sequences of hyphens like --- or ==\n",
                "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
                "    # sequences of white spaces\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    return text.strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "clean_text = clean(text)\n",
                "print(clean_text)\n",
                "print(\"Impurity:\", impurity(clean_text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "# just for formatting - ignore\n",
                "import textwrap\n",
                "\n",
                "for line in textwrap.wrap(clean_text):\n",
                "    print(line)\n",
                "print(\"Impurity:\", impurity(clean_text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "df['clean_text'] = df['text'].progress_apply(clean)\n",
                "df['impurity']   = df['clean_text'].apply(impurity, min_len=20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "# just for formatting in the book - ignore\n",
                "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False).head(3) \\\n",
                ".applymap(lambda x: x if type(x) == float else x[:80]+'...')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Removing Noise with textacy \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from textacy.preprocessing.resources import RE_URL\n",
                "\n",
                "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2020-01-27T21:14:23.976684Z",
                    "start_time": "2020-01-27T21:14:23.824416Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Pattern-based Data Masking with Textacy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from textacy.preprocessing.replace import replace_urls\n",
                "\n",
                "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
                "\n",
                "# using default substitution _URL_\n",
                "print(replace_urls(text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Unicode Character Normalization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "text = \"The caf\u00e9 \u201cSaint-Rapha\u00ebl\u201d is loca-\\nted on C\u00f4te d\u02bcAzur.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import textacy.preprocessing as tprep\n",
                "\n",
                "def normalize(text):\n",
                "    text = tprep.normalize_hyphenated_words(text)\n",
                "    text = tprep.normalize_quotation_marks(text)\n",
                "    text = tprep.normalize_unicode(text)\n",
                "    text = tprep.remove_accents(text)\n",
                "    return text\n",
                "\n",
                "print(normalize(text))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df['clean_text'] = df['clean_text'].progress_map(normalize)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "df['text'] = df['clean_text']\n",
                "df.drop(columns=['clean_text', 'impurity'], inplace=True)\n",
                "\n",
                "db_path = \"../data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\n",
                "con.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Tokenization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "text = \"\"\"\n",
                "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
                "solution for today's problem. The code of module AC68 should be -1. \n",
                "Have to think a bit... #goodnight ;-) \ud83d\ude29\ud83d\ude2c\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Tokenization with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "tokens = re.findall(r'\\w\\w+', text)\n",
                "print(\"|\".join(tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "# just for formatting - ignore\n",
                "import textwrap\n",
                "\n",
                "for line in textwrap.wrap(\" \".join(re.findall(r'\\w\\w+', text))):\n",
                "    print(line.replace(\" \", \"|\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "RE_TOKEN = re.compile(r\"\"\"\n",
                "               ( [#]?[@\\w'\u2019\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
                "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
                "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
                "               )\n",
                "               \"\"\", re.VERBOSE)\n",
                "\n",
                "def tokenize(text):\n",
                "    return RE_TOKEN.findall(text)\n",
                "\n",
                "tokens = tokenize(text)\n",
                "print(\"|\".join(tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "# just for formatting - ignore\n",
                "for line in textwrap.wrap(\" \".join(tokens)):\n",
                "    print(line.replace(\" \", \"|\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "df['tokens'] = df['text'].progress_map(tokenize)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Tokenization with NLTK\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import nltk\n",
                "\n",
                "nltk.download('punkt') ###\n",
                "tokens = nltk.tokenize.word_tokenize(text)\n",
                "print(\"|\".join(t for t in tokens))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "# just for formatting - ignore\n",
                "for line in textwrap.wrap(\" \".join(tokens)):\n",
                "    print(line.replace(\" \", \"|\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Recommendations for Tokenization\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Linguistic Processing with spaCy\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Instantiating a Pipeline\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import spacy\n",
                "nlp = spacy.load('en')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp.pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Processing Text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp = spacy.load(\"en\")\n",
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "for token in doc:\n",
                "    print(token, end=\"|\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "from blueprints.preparation import display_nlp\n",
                "display_nlp(doc)\n",
                "### Table tab-nlp-result: Result of spaCy's document processing as generated by `display_nlp`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def display_nlp(doc, include_punct=False):\n",
                "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
                "\n",
                "    rows = []\n",
                "    for i, t in enumerate(doc):\n",
                "        if not t.is_punct or include_punct:\n",
                "            row = {'token': i, \n",
                "                   'text': t.text, 'lemma': t.lemma_, \n",
                "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
                "                   'pos': t.pos_, 'dep': t.dep_, \n",
                "                   'ent_type': t.ent_type_}\n",
                "            rows.append(row)\n",
                "    \n",
                "    df = pd.DataFrame(rows).set_index('token')\n",
                "    df.index.name = None\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Modifying Tokenization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) \ud83d\ude0b\ud83d\udc4d\"\n",
                "nlp = spacy.load('en') ###\n",
                "doc = nlp(text)\n",
                "\n",
                "print(*[token for token in doc], sep=\"|\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import re ###\n",
                "import spacy ###\n",
                "from spacy.tokenizer import Tokenizer\n",
                "from spacy.util import compile_prefix_regex, \\\n",
                "                       compile_infix_regex, compile_suffix_regex\n",
                "\n",
                "def custom_tokenizer(nlp):\n",
                "    \n",
                "    # use default patterns except the ones matched by re.search\n",
                "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
                "                if pattern not in ['-', '_', '#']]\n",
                "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
                "                if pattern not in ['_']]\n",
                "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
                "                if not re.search(pattern, 'xx-xx')]\n",
                "\n",
                "    return Tokenizer(vocab          = nlp.vocab, \n",
                "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
                "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
                "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
                "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
                "                     token_match    = nlp.Defaults.token_match)\n",
                "\n",
                "nlp = spacy.load('en')\n",
                "nlp.tokenizer = custom_tokenizer(nlp)\n",
                "\n",
                "doc = nlp(text)\n",
                "print(*[token for token in doc], sep=\"|\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Lemmatization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "stemmer = nltk.snowball.SnowballStemmer(\"english\")\n",
                "\n",
                "words = \"university universe easily easy\"\n",
                "\n",
                "for word in words.split():\n",
                "    print(f\"{word:>12} --> {stemmer.stem(word)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Stop Word Detection\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from spacy.lang.en import STOP_WORDS as stop_words\n",
                "print(len(stop_words))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp = spacy.load('en')\n",
                "nlp.vocab['down'].is_stop = False\n",
                "nlp.vocab['Dear'].is_stop = True\n",
                "nlp.vocab['Regards'].is_stop = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
                "doc = nlp.make_doc(text) # only tokenize\n",
                "    \n",
                "tokens_wo_stop = [token for token in doc ]\n",
                "for token in doc:\n",
                "    if not token.is_stop and not token.is_punct:\n",
                "        print(token, end='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Part-of-Speech Tagging\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Blueprints for Feature Extraction\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Extracting Words based on Part-of-Speech\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import textacy\n",
                "\n",
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "tokens = textacy.extract.words(doc, \n",
                "            filter_stops = True,           # default True, no stopwords\n",
                "            filter_punct = True,           # default True, no punctuation\n",
                "            filter_nums = True,            # default False, no numbers\n",
                "            include_pos = ['ADJ', 'NOUN'], # default None = include all\n",
                "            exclude_pos = None,            # default None = exclude none\n",
                "            min_freq = 1)                  # minimum frequency of words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def extract_lemmas(doc, **kwargs):\n",
                "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
                "\n",
                "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
                "print(*lemmas, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Extracting Noun Chunks\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "fragment"
                }
            },
            "outputs": [],
            "source": [
                "def ngrams(tokens, n=2, sep=' '):\n",
                "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]\n",
                "\n",
                "# just for formatting - ignore\n",
                "for t in textwrap.wrap(' '.join(ngrams(tokenize(text), sep='_'))):\n",
                "    print(t.replace(' ', '|'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "spans = textacy.extract.matches(doc, patterns=[\"POS:ADJ:? POS:NOUN:+\"])\n",
                "print(*spans, sep='|')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "print(*doc.noun_chunks, sep='|')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def extract_noun_chunks(doc, include_pos=['NOUN'], sep='_'):\n",
                "\n",
                "    chunks = []\n",
                "    for noun_chunk in doc.noun_chunks:\n",
                "        chunk = [token.lemma_ for token in noun_chunk\n",
                "                 if token.pos_ in include_pos]\n",
                "        if len(chunk) >= 2:\n",
                "            chunks.append(sep.join(chunk))\n",
                "    return chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "noun_chunks = extract_noun_chunks(doc, include_pos=['ADJ', 'NOUN', 'PROPN'])\n",
                "print(*noun_chunks, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-08-18T10:52:30.491608Z",
                    "start_time": "2019-08-18T10:52:30.486644Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Extracting Named Entities\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def extract_entities(doc, include_types=None, sep='_'):\n",
                "\n",
                "    ents = textacy.extract.entities(doc, \n",
                "             include_types=include_types, \n",
                "             exclude_types=None, \n",
                "             drop_determiners=True, \n",
                "             min_freq=1)\n",
                "    \n",
                "    return [re.sub('\\s+', sep, e.lemma_)+'/'+e.label_ for e in ents]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp = spacy.load('en') ###\n",
                "text = \"George Washington was the first president of the United States.\"\n",
                "doc = nlp(text)\n",
                "\n",
                "entities = extract_entities(doc, ['PERSON', 'GPE'])\n",
                "print(*entities, sep='|')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Extracting NLP Features on a Large Dataset\n",
                "## One Function to Get It All\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "nlp = spacy.load('en') # load model\n",
                "nlp.tokenizer = custom_tokenizer(nlp) # optional\n",
                "\n",
                "def nlp_extract(text):\n",
                "\n",
                "    doc = nlp(text)\n",
                "    \n",
                "    lemmas          = extract_lemmas(doc, exclude_pos = ['PART', 'PUNCT', \n",
                "                                           'DET', 'PRON', 'SYM', 'SPACE'],\n",
                "                                          filter_stops = False)\n",
                "    adjs_verbs      = extract_lemmas(doc, include_pos = ['ADJ', 'VERB'])\n",
                "    nouns           = extract_lemmas(doc, include_pos = ['NOUN', 'PROPN'])\n",
                "    noun_chunks     = extract_noun_chunks(doc, ['NOUN'])\n",
                "    adj_noun_chunks = extract_noun_chunks(doc, ['NOUN', 'ADJ'])\n",
                "    entities        = extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
                "\n",
                "    return lemmas, adjs_verbs, nouns, noun_chunks, adj_noun_chunks, entities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
                "print(*nlp_extract(text), sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Creating Multiple Columns in a Data Frame\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "db_path = \"../data/reddit-selfposts/reddit-selfposts.db\" ###\n",
                "con = sqlite3.connect(db_path)\n",
                "df = pd.read_sql(\"select * from posts_cleaned\", con)\n",
                "con.close()\n",
                "\n",
                "df['text'] = df['title'] + ': ' + df['text']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# for faster processing\n",
                "# df = df.sample(500) ###\n",
                "# len(df) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "# define column names\n",
                "nlp_columns = ['lemmas', 'adjs_verbs', 'nouns', 'noun_chunks', \n",
                "               'adj_noun_chunks', 'entities']\n",
                "\n",
                "### this takes about 10-15 min\n",
                "df[nlp_columns] = df.progress_apply(lambda row: nlp_extract(row['text']), \n",
                "                                    axis='columns', result_type='expand')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "count_words(df, 'noun_chunks').head(10).plot(kind='barh', figsize=(8,3)).invert_yaxis()\n",
                "### img_width: 80%"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Persisting the Result\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import sqlite3 ###\n",
                "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
                "\n",
                "con = sqlite3.connect(db_path) \n",
                "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
                "con.close() "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### A Note on Execution Time\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# There is More\n",
                "## Language Detection\n",
                "## Spell Checking\n",
                "## Token Normalization\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Closing Remarks and Recommendations\n"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "265.638px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
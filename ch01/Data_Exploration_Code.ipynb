{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Remark<div class='tocSkip'/>\n",
                "\n",
                "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
                "\n",
                "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
                "\n",
                "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
                "\n",
                "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup<div class='tocSkip'/>\n",
                "\n",
                "## Determine Environment<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "ON_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if ON_COLAB:\n",
                "    BASE_DIR = \"/content\"\n",
                "    print(\"You are working on Google Colab.\")\n",
                "    print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
                "    # adjust release\n",
                "    GIT_ROOT = \"https://github.com/blueprints-for-text-analytics-python/early-release/raw/master\"\n",
                "else:\n",
                "    BASE_DIR = \"../\"\n",
                "    print(\"You are working on a local system.\")\n",
                "    print(f'Files will be searched relative to \"{BASE_DIR}\".')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download files on Google Colab<div class='tocSkip'/>\n",
                "\n",
                "If you are on Colab, copy the following statements into the code cell below and execute them.\n",
                "\n",
                "```bash\n",
                "!wget -P $BASE_DIR $GIT_ROOT/settings.py\n",
                "\n",
                "!mkdir -p $BASE_DIR/data/un-general-debates\n",
                "\n",
                "!wget -P $BASE_DIR/data/un-general-debates $GIT_ROOT/data/un-general-debates/un-general-debates-blueprint.csv.gz \n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install required libraries<div class='tocSkip'/>\n",
                "\n",
                "Still todo: setup pip requirements.txt\n",
                "\n",
                "If you are on Colab, copy the following statements into the code cell below and execute them.\n",
                "\n",
                "```bash\n",
                "!pip install textacy\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "# make sure stop words are available\n",
                "nltk.download('stopwords')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Python Settings<div class=\"tocSkip\"/>\n",
                "\n",
                "Common imports, defaults for formatting in Matplotlib, Pandas etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'png'\n",
                "\n",
                "%run \"$BASE_DIR/settings.py\"\n",
                "\n",
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gaining Early Insights from Textual Data\n",
                "## What you will learn and what we will build\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploratory Data Analysis\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introducing the Dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file = f\"{BASE_DIR}/data/un-general-debates/un-general-debates-blueprint.csv.gz\"\n",
                "df = pd.read_csv(file)\n",
                "df.sample(2, random_state=53)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Getting an Overview of the Data with Pandas\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calculating Summary Statistics for Columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['length'] = df['text'].str.len()\n",
                "\n",
                "df.describe().T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[['country', 'speaker']].describe(include='O').T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Checking for Missing Data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['speaker'].fillna('unkown', inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[df['speaker'].str.contains('Bush')]['speaker'].value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plotting Value Distributions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['length'].plot(kind='box', vert=False, figsize=(8, 1))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['length'].plot(kind='hist', bins=30, figsize=(8,2))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Not in book: seaborn plot with gaussian kernel density estimate\n",
                "import seaborn as sns\n",
                "\n",
                "plt.figure(figsize=(8, 2))\n",
                "sns.distplot(df['length'], bins=30, kde=True);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing Value Distributions across Categories\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "where = df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS'])\n",
                "sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='box', ax=axes[0])\n",
                "sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='violin', ax=axes[1])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Developments over Time\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.groupby('year').size().plot(title=\"Number of Countries\", figsize=(6,2))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.groupby('year').agg({'length': 'mean'}) \\\n",
                "  .plot(title=\"Avg. Speech Length\", ylim=(0,30000), figsize=(6,2))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "df.groupby('year').size().plot(title=\"Number of Countries\", ax=axes[0])\n",
                "df.groupby('year').agg({'length': 'mean'}).plot(title=\"Avg. Speech Length\", ax=axes[1], ylim=(0,30000))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Building a Simple Text Preprocessing Pipeline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization with Regular Expressions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import regex as re\n",
                "\n",
                "def tokenize(text):\n",
                "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"Let's defeat SARS-CoV-2 together in 2020!\"\n",
                "tokens = tokenize(text)\n",
                "print(\"|\".join(tokens))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Treating Stop Words\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "\n",
                "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
                "\n",
                "def remove_stop(tokens):\n",
                "    return [t for t in tokens if t.lower() not in stopwords]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "include_stopwords = {'dear', 'regards', 'must', 'would', 'also'}\n",
                "exclude_stopwords = {'against'}\n",
                "\n",
                "stopwords |= include_stopwords\n",
                "stopwords -= exclude_stopwords"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Processing a Pipeline with one Line of Code\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline = [str.lower, tokenize, remove_stop]\n",
                "\n",
                "def prepare(text, pipeline):\n",
                "    tokens = text\n",
                "    for transform in pipeline:\n",
                "        tokens = transform(tokens)\n",
                "    return tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['tokens'] = df['text'].progress_apply(prepare, pipeline=pipeline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['no_tokens'] = df['tokens'].progress_map(len)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analyzing Word Frequencies\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Counting Words with a Counter\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "tokens = tokenize(\"She likes my cats and my cats like my sofa.\")\n",
                "\n",
                "counter = Counter(tokens)\n",
                "print(counter)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "more_tokens = tokenize(\"She likes dogs and cats.\")\n",
                "counter.update(more_tokens)\n",
                "\n",
                "print(counter)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "counter = Counter()\n",
                "\n",
                "_ = df['tokens'].map(counter.update)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pp.pprint(counter.most_common(5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
                "\n",
                "    # process tokens and update counter\n",
                "    def update(doc):\n",
                "        tokens = doc if preprocess is None else preprocess(doc)\n",
                "        counter.update(tokens)\n",
                "\n",
                "    # create counter and run through all data\n",
                "    counter = Counter()\n",
                "    df[column].progress_map(update)\n",
                "\n",
                "    # transform counter into data frame\n",
                "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
                "    freq_df = freq_df.query('freq >= @min_freq')\n",
                "    freq_df.index.name = 'token'\n",
                "    \n",
                "    return freq_df.sort_values('freq', ascending=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_df = count_words(df)\n",
                "freq_df.head(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Creating a Frequency Diagram\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ax = freq_df.head(15).plot(kind='barh', width=0.95, figsize=(8,3))\n",
                "ax.invert_yaxis()\n",
                "ax.set(xlabel='Frequency', ylabel='Token', title='Top Words')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Creating Word Clouds\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from wordcloud import WordCloud\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "text = df.query(\"year==2015 and country=='USA'\")['text'].values[0]\n",
                "\n",
                "wc = WordCloud(max_words=100, stopwords=stopwords)\n",
                "wc.generate(text)\n",
                "plt.imshow(wc, interpolation='bilinear')\n",
                "plt.axis(\"off\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
                "\n",
                "    wc = WordCloud(width=800, height=400, \n",
                "                   background_color= \"black\", colormap=\"Paired\", \n",
                "                   max_font_size=150, max_words=max_words)\n",
                "    \n",
                "    # convert data frame into dict\n",
                "    if type(word_freq) == pd.Series:\n",
                "        counter = Counter(word_freq.fillna(0).to_dict())\n",
                "    else:\n",
                "        counter = word_freq\n",
                "\n",
                "    # filter stop words in frequency counter\n",
                "    if stopwords is not None:\n",
                "        counter = {token:freq for (token, freq) in counter.items() \n",
                "                              if token not in stopwords}\n",
                "    wc.generate_from_frequencies(counter)\n",
                " \n",
                "    plt.title(title) \n",
                "\n",
                "    plt.imshow(wc, interpolation='bilinear')\n",
                "    plt.axis(\"off\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_2015_df = count_words(df[df['year']==2015])\n",
                "plt.figure(figsize=(12,4))\n",
                "wordcloud(freq_2015_df['freq'], max_words=100)\n",
                "wordcloud(freq_2015_df['freq'], max_words=100, stopwords=freq_df.head(50).index)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Ranking with TF-IDF\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def idf(df, column='tokens', preprocess=None, min_df=2):\n",
                "\n",
                "    def update(doc):\n",
                "        tokens = doc if preprocess is None else preprocess(doc)\n",
                "        counter.update(set(tokens))\n",
                "\n",
                "    # count tokens\n",
                "    counter = Counter()\n",
                "    df[column].progress_map(update)\n",
                "\n",
                "    # create data frame and compute idf\n",
                "    idf_df = pd.DataFrame.from_dict(counter, orient='index', columns=['df'])\n",
                "    idf_df = idf_df[idf_df['df'] >= min_df]\n",
                "    idf_df['idf'] = np.log(len(df)/idf_df['df'])+0.1\n",
                "    idf_df.index.name = 'token'\n",
                "    return idf_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "idf_df = idf(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_df = freq_df.join(idf_df)\n",
                "freq_df['tfidf'] = freq_df['freq'] * freq_df['idf']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_1970 = count_words(df[df['year'] == 1970])\n",
                "freq_2015 = count_words(df[df['year'] == 2015])\n",
                "\n",
                "freq_1970['tfidf'] = freq_1970['freq'] * idf_df['idf']\n",
                "freq_2015['tfidf'] = freq_2015['freq'] * idf_df['idf']\n",
                "\n",
                "#wordcloud(freq_df['freq'], title='All years', subplot=(1,3,1))\n",
                "wordcloud(freq_1970['freq'], title='1970 - TF', \n",
                "          stopwords=['twenty-fifth', 'twenty-five'])\n",
                "wordcloud(freq_2015['freq'], title='2015 - TF', \n",
                "          stopwords=['seventieth'])\n",
                "wordcloud(freq_1970['tfidf'], title='1970 - TF-IDF', \n",
                "          stopwords=['twenty-fifth', 'twenty-five', 'twenty', 'fifth'])\n",
                "wordcloud(freq_2015['tfidf'], title='2015 - TF-IDF', \n",
                "          stopwords=['seventieth'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Finding a Keyword in Context (KWIC)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from textacy.text_utils import KWIC\n",
                "\n",
                "def kwic(doc_series, keyword, window=35, print_samples=None):\n",
                "\n",
                "    def add_kwic(text):\n",
                "        kwic_list.extend(KWIC(text, keyword, ignore_case=True, \n",
                "                              window_width=window, print_only=False))\n",
                "\n",
                "    kwic_list = []\n",
                "    doc_series.progress_map(add_kwic)\n",
                "\n",
                "    if print_samples is None or print_samples==0:\n",
                "        return kwic_list\n",
                "    else:\n",
                "        k = min(print_samples, len(kwic_list))\n",
                "        print(f\"{k} random samples out of {len(kwic_list)} \" + \\\n",
                "              f\"contexts for '{keyword}':\")\n",
                "        for sample in random.sample(list(kwic_list), k):\n",
                "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
                "                  sample[1]+'  '+\\\n",
                "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic(df[df['year'] == 2015]['text'], 'sdgs', window=35, print_samples=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from textacy.text_utils import KWIC\n",
                "\n",
                "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
                "\n",
                "    def add_kwic(text):\n",
                "        kwic_list.extend(KWIC(text, keyword, ignore_case=True, \n",
                "                              window_width=window, print_only=False))\n",
                "\n",
                "    kwic_list = []\n",
                "    doc_series.progress_map(add_kwic)\n",
                "\n",
                "    if print_samples is None or print_samples==0:\n",
                "        return kwic_list\n",
                "    else:\n",
                "        k = min(print_samples, len(kwic_list))\n",
                "        print(f\"{k} random samples out of {len(kwic_list)} \" + \\\n",
                "              f\"contexts for '{keyword}':\")\n",
                "        for sample in random.sample(list(kwic_list), k):\n",
                "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
                "                  sample[1]+'  '+\\\n",
                "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic(df[df['year'] == 2015]['text'], 'sdgs', print_samples=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Analyzing N-Grams\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"the visible manifestation of the global climate change\"\n",
                "tokens = tokenize(text)\n",
                "\n",
                "def ngrams(tokens, n=2, sep=' '):\n",
                "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]\n",
                "\n",
                "print(\"|\".join(ngrams(tokens, 2)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ngrams(tokens, n=2, sep=' ', stopwords=set()):\n",
                "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])\n",
                "            if len([t for t in ngram if t in stopwords])==0]\n",
                "\n",
                "tokens = prepare(text, [str.lower, tokenize]) # keep full list of tokens\n",
                "\n",
                "print(\"Bigrams:\", \"|\".join(ngrams(tokens, 2, stopwords=stopwords)))\n",
                "print(\"Trigrams:\", \"|\".join(ngrams(tokens, 3, stopwords=stopwords)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['bigrams'] = df['text'].progress_apply(prepare, pipeline=[str.lower, tokenize]) \\\n",
                "                          .progress_apply(ngrams, n=2, stopwords=stopwords)\n",
                "\n",
                "count_words(df, 'bigrams').head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# concatenate existing IDF data frame with bigram IDFs\n",
                "idf_df = pd.concat([idf_df, idf(df, 'bigrams', min_df=10)])\n",
                "\n",
                "freq_df = count_words(df[df['year'] == 2015], 'bigrams')\n",
                "freq_df['tfidf'] = freq_df['freq'] * idf_df['idf']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wordcloud(freq_df['tfidf'], title='all bigrams', max_words=50)\n",
                "\n",
                "where = freq_df.index.str.contains('climate')\n",
                "wordcloud(freq_df[where]['freq'], title='\"climate\" bigrams', max_words=50)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Comparing Frequencies across Time-Intervals and Categories\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Frequency Timelines\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_keywords(tokens, keywords): \n",
                "    tokens = [t for t in tokens if t in keywords]\n",
                "    counter = Counter(tokens)\n",
                "    return [counter.get(k, 0) for k in keywords]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keywords = ['nuclear', 'terrorism', 'climate', 'freedom']\n",
                "tokens = ['nuclear', 'climate', 'climate', 'freedom', 'climate', 'freedom']\n",
                "\n",
                "print(count_keywords(tokens, keywords))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_keywords_by(df, by, column='tokens', keywords=keywords):\n",
                "    \n",
                "    freq_matrix = df['tokens'].progress_apply(count_keywords, keywords=keywords)\n",
                "    freq_df = pd.DataFrame.from_records(freq_matrix, columns=keywords)\n",
                "    freq_df[by] = df[by] # copy the grouping column(s)\n",
                "    \n",
                "    return freq_df.groupby(by=by).sum().sort_values(by)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_df = count_keywords_by(df, by='year', keywords=keywords)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.options.display.max_rows = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.options.display.max_rows = 60"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freq_df.plot(kind='line', figsize=(8, 3))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# analyzing mentions of 'climate' before 1980\n",
                "kwic(df.query('year < 1980')['text'], 'climate', window=35, print_samples=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Frequency Heat Maps\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keywords = ['terrorism', 'terrorist', 'nuclear', 'war', 'oil',\n",
                "            'syria', 'syrian', 'refugees', 'migration', 'peacekeeping', \n",
                "            'humanitarian', 'climate', 'change', 'sustainable', 'sdgs']  \n",
                "\n",
                "freq_df = count_keywords_by(df, by='year', keywords=keywords)\n",
                "\n",
                "# compute relative frequencies based on total number of tokens per year\n",
                "freq_df = freq_df.div(df.groupby('year')['no_tokens'].sum(), axis=0)\n",
                "# apply square root as sublinear filter for better contrast\n",
                "freq_df = freq_df.apply(np.sqrt)\n",
                "\n",
                "sns.heatmap(data=freq_df.T, \n",
                "            xticklabels=True, yticklabels=True, cbar=False, cmap=\"Reds\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info(memory_usage='deep')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Closing Remarks\n"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "281.562px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
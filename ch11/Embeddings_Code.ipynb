{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "# Remark<div class='tocSkip'/>\n",
                "\n",
                "The code in this notebook differs slightly from the printed book, because we removed some boilerplate parts of it. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
                "\n",
                "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book. Numbers in the book may have less decimal places as shown here in the notebook.\n",
                "\n",
                "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
                "\n",
                "All of this is done to simplify the code in the book and put the focus on the important parts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "a3UVlFeb3-j1",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "# Setup<div class='tocSkip'/>\n",
                "\n",
                "## Determine Environment<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 52
                },
                "colab_type": "code",
                "id": "Z5CvQu904xx4",
                "outputId": "445c085e-16ea-4515-c059-1cb9edc3f374",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "import sys\n",
                "ON_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if ON_COLAB:\n",
                "    BASE_DIR = \"/content\"\n",
                "    print(\"You are working on Google Colab.\")\n",
                "    print(f'Files will be downloaded to \"{BASE_DIR}\".')\n",
                "    # adjust release\n",
                "    GIT_ROOT = \"https://github.com/blueprints-for-text-analytics-python/early-release/raw/master\"\n",
                "else:\n",
                "    BASE_DIR = \"..\"\n",
                "    print(\"You are working on a local system.\")\n",
                "    print(f'Files will be searched relative to \"{BASE_DIR}\".')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Download data files<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "import os, subprocess\n",
                "from subprocess import PIPE\n",
                "\n",
                "required_files = [\n",
                "                  'settings.py',\n",
                "                  'packages/blueprints/__init__.py',\n",
                "                  'packages/blueprints/embeddings.py',\n",
                "                  'data/reddit-selfposts/reddit-selfposts.db.gz',\n",
                "                  'ch11/colab_requirements.txt'\n",
                "]\n",
                "\n",
                "if ON_COLAB:\n",
                "    print(\"Downloading required files ...\")\n",
                "    for file in required_files:\n",
                "        cmd = ['wget', '-P', os.path.dirname(BASE_DIR+'/'+file), GIT_ROOT+'/'+file]\n",
                "        print('!'+' '.join(cmd))\n",
                "        stdout, stderr = subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE).communicate()\n",
                "        # print(stderr.decode()) # uncomment in case of problems"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "XR2aPCyiCAJL",
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Install required libraries and additional setup<div class='tocSkip'/>\n",
                "\n",
                "It may take a moment to install the required Python libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "if ON_COLAB:\n",
                "    print(\"\\nAdditional setup ...\")\n",
                "    setup_cmds = ['pip install -r ch11/colab_requirements.txt',\n",
                "                  'mkdir -p models',\n",
                "                  f'gunzip -k {BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db.gz']\n",
                "\n",
                "    for cmd in setup_cmds:\n",
                "        print('!'+cmd)\n",
                "        if os.system(cmd) != 0:\n",
                "            print('  --> ERROR')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "## Common Imports<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'png'\n",
                "\n",
                "%run \"$BASE_DIR/settings.py\"\n",
                "\n",
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "InteractiveShell.ast_node_interactivity = \"last_expr\" # in this notebook not \"all\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# to import blueprints package\n",
                "import os, sys\n",
                "sys.path.append(BASE_DIR + '/packages')\n",
                "# sys.path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# for Gensim training\n",
                "\n",
                "import logging\n",
                "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n",
                "\n",
                "logging.getLogger().setLevel(logging.WARNING)\n",
                "logging.getLogger().info(\"Logging INFOS.\")\n",
                "logging.getLogger().warning(\"Logging WARNINGS.\")\n",
                "logging.getLogger().error(\"Logging ERRORS.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
                "from gensim.scripts.glove2word2vec import glove2word2vec\n",
                "\n",
                "from gensim.models.phrases import Phrases, npmi_scorer\n",
                "from gensim.models.word2vec import LineSentence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# set precision for similarity values\n",
                "%precision 3\n",
                "np.set_printoptions(suppress=True) # no scientific for small numbers\n",
                "\n",
                "# for figure cropping and conversion\n",
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Exploring Semantic Relationships with Word Embeddings\n",
                "## What you will learn and what we will build\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-08-12T15:39:13.555307Z",
                    "start_time": "2019-08-12T15:39:13.154307Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# The Case for Semantic Embeddings\n",
                "## Word Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2019-08-12T15:39:13.555307Z",
                    "start_time": "2019-08-12T15:39:13.154307Z"
                },
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Analogy Reasoning with Word Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Types of Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Word2Vec\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### GloVe\n",
                "### FastText\n",
                "### Deep Contextualized Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Blueprint: Similarity Queries on Pre-Trained Models\n",
                "## Loading a Pretrained Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import os ###\n",
                "os.environ['GENSIM_DATA_DIR'] = './models'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# not in the book: display models as table\n",
                "pd.options.display.float_format = '{:.0f}'.format ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import gensim.downloader as api\n",
                "\n",
                "info_df = pd.DataFrame.from_dict(api.info()['models'], orient='index')\n",
                "info_df[['file_size', 'base_dataset', 'parameters']].head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# full list of columns\n",
                "info_df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "pd.options.display.float_format = '{:.2f}'.format ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model = api.load(\"glove-wiki-gigaword-50\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Similarity Queries\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "%precision 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "v_king = model['king']\n",
                "v_queen = model['queen']\n",
                "\n",
                "print(\"Vector size:\", model.vector_size)\n",
                "print(\"v_king  =\", v_king[:10])\n",
                "print(\"v_queen =\", v_queen[:10])\n",
                "print(\"similarity:\", model.similarity('king', 'queen'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "%precision 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.most_similar('king', topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "v_lion = model['lion']\n",
                "v_nano = model['nanotechnology']\n",
                "\n",
                "model.cosine_similarities(v_king, [v_queen, v_lion, v_nano])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.most_similar(positive=['paris', 'germany'], negative=['france'], topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.most_similar(positive=['france', 'capital'], topn=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.most_similar(positive=['greece', 'capital'], topn=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Blueprint: Training and Comparing your own Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Data Preparation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "db_path = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts.db\"\n",
                "con = sqlite3.connect(db_path)\n",
                "df = pd.read_sql(\"select subreddit, lemmas, text from posts_nlp\", con)\n",
                "con.close()\n",
                "\n",
                "df['lemmas'] = df['lemmas'].str.lower().str.split() # lower case tokens\n",
                "sents = df['lemmas'] # our training \"sentences\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Phrases\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from gensim.models.phrases import Phrases, npmi_scorer\n",
                "\n",
                "phrases = Phrases(sents, min_count=10, threshold=0.3, \n",
                "                  delimiter=b'-', scoring=npmi_scorer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "sent = \"I had to replace the timing belt in my mercedes c300\".split()\n",
                "phrased = phrases[sent]\n",
                "print('|'.join(phrased))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "phrase_df = pd.DataFrame(phrases.export_phrases(sents), \n",
                "                         columns =['phrase', 'score'])\n",
                "phrase_df = phrase_df[['phrase', 'score']].drop_duplicates() \\\n",
                "            .sort_values(by='score', ascending=False).reset_index(drop=True)\n",
                "phrase_df['phrase'] = phrase_df['phrase'].map(lambda p: p.decode('utf-8'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "phrase_df[phrase_df['phrase'].str.contains('mercedes')] .head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# show some additional phrases with score > 0.7\n",
                "phrase_df.query('score > 0.7').sample(100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "logging.getLogger().setLevel(logging.WARNING) ###\n",
                "phrases = Phrases(sents, min_count=10, threshold=0.7, \n",
                "                  delimiter=b'-', scoring=npmi_scorer)\n",
                "\n",
                "df['phrased_lemmas'] = df['lemmas'].progress_map(lambda s: phrases[s])\n",
                "sents = df['phrased_lemmas']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Training\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "logging.getLogger().setLevel(logging.INFO) ###\n",
                "model = Word2Vec(sents,       # tokenized input sentences\n",
                "                 size=100,    # size of word vectors (default 100)\n",
                "                 window=2,    # context window size (default 5)\n",
                "                 sg=1,        # use skip-gram (default 0 = CBOW)\n",
                "                 negative=5,  # number of negative samples (default 5)\n",
                "                 min_count=5, # ignore infrequent words (default 5)\n",
                "                 workers=4,   # number of threads (default 3)\n",
                "                 iter=5)      # number of epochs (default 5)\n",
                "logging.getLogger().setLevel(logging.ERROR) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "model.save('./models/autos_w2v_100_2_full.bin')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "model = Word2Vec.load('./models/autos_w2v_100_2_full.bin')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "source": [
                "**This takes several minutes on Colab.** Please be patient, you need this to continue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "from gensim.models import Word2Vec, FastText\n",
                "\n",
                "model_path = './models'\n",
                "model_prefix = 'autos'\n",
                "\n",
                "param_grid = {'w2v': {'variant': ['cbow', 'sg'], 'window': [2, 5, 30]},\n",
                "              'ft': {'variant': ['sg'], 'window': [5]}}\n",
                "size = 100\n",
                "\n",
                "for algo, params in param_grid.items(): \n",
                "    print(algo) ###\n",
                "    for variant in params['variant']:\n",
                "        sg = 1 if variant == 'sg' else 0\n",
                "        for window in params['window']:\n",
                "            print(f\"  Variant: {variant}, Window: {window}, Size: {size}\") ###\n",
                "            np.random.seed(1) ### to ensure repeatability\n",
                "            if algo == 'w2v':\n",
                "                model = Word2Vec(sents, size=size, window=window, sg=sg)\n",
                "            else:\n",
                "                model = FastText(sents, size=size, window=window, sg=sg)\n",
                "\n",
                "            file_name = f\"{model_path}/{model_prefix}_{algo}_{variant}_{window}\"\n",
                "            model.wv.save_word2vec_format(file_name + '.bin', binary=True) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Evaluating Different Models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "### You can add the other computed models as well here.\n",
                "### For the book we just selected five of them. \n",
                "from gensim.models import KeyedVectors\n",
                "model_path = './models' ###\n",
                "\n",
                "names = ['autos_w2v_cbow_2', 'autos_w2v_sg_2', \n",
                "         'autos_w2v_sg_5', 'autos_w2v_sg_30', 'autos_ft_sg_5']\n",
                "models = {}\n",
                "\n",
                "for name in names:\n",
                "    file_name = f\"{model_path}/{name}.bin\"\n",
                "    print(f\"Loading {file_name}\") ###\n",
                "    models[name] = KeyedVectors.load_word2vec_format(file_name, binary=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "def compare_models(models, **kwargs):\n",
                "\n",
                "    df = pd.DataFrame()\n",
                "    for name, model in models:\n",
                "        df[name] = [f\"{word} {score:.3f}\" \n",
                "                    for word, score in model.most_similar(**kwargs)]\n",
                "    df.index = df.index + 1 # let row index start at 1\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "compare_models([(n, models[n]) for n in names], positive='bmw', topn=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Looking for Similar Concepts\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### Analogy Reasoning on our own Models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['f150', 'toyota'], negative=['ford'], topn=5).T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# try a different analogy\n",
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['x3', 'mercedes'], negative=['bmw'], topn=5).T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# and another one\n",
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['spark-plug'], negative=[], topn=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Visualizing Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Blueprint: Applying Dimensionality Reduction\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "from umap import UMAP\n",
                "\n",
                "model = models['autos_w2v_sg_30']\n",
                "words = model.vocab\n",
                "wv = [model[word] for word in words]\n",
                "\n",
                "reducer = UMAP(n_components=2, metric='cosine', n_neighbors = 15, min_dist=0.1,\n",
                "               random_state = 12)\n",
                "reduced_wv = reducer.fit_transform(wv)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import plotly.express as px\n",
                "\n",
                "df = pd.DataFrame.from_records(reduced_wv, columns=['x', 'y'])\n",
                "df['word'] = words\n",
                "params = {'hover_data': {c: False for c in df.columns}, 'hover_name': 'word'}\n",
                "params.update({'width': 800, 'height': 600}) ###\n",
                "\n",
                "fig = px.scatter(df, x=\"x\", y=\"y\", opacity=0.3, size_max=3, **params)\n",
                "fig.update_traces(marker={'line': {'width': 0}}) ###\n",
                "fig.update_xaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
                "fig.update_yaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "from blueprints.embeddings import plot_embeddings\n",
                "\n",
                "model = models['autos_w2v_sg_30'] ###\n",
                "search = ['ford', 'lexus', 'audi', 'vw', 'hyundai', \n",
                "          'goodyear', 'spark-plug', 'florida', 'navigation']\n",
                "\n",
                "_ = plot_embeddings(model, search, topn=50, show_all=True, labels=False, \n",
                "                algo='umap', n_neighbors=15, min_dist=0.1, random_state=12)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "model = models['autos_w2v_sg_30'] ###\n",
                "search = ['ford', 'bmw', 'toyota', 'tesla', 'audi', 'mercedes', 'hyundai']\n",
                "\n",
                "_ = plot_embeddings(model, search, topn=10, show_all=False, labels=True, \n",
                "    algo='umap', n_neighbors=15, min_dist=10, spread=20, random_state=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "_ = plot_embeddings(model, search, topn=30, n_dims=3, \n",
                "    algo='umap', n_neighbors=15, min_dist=.1, spread=40, random_state=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "# PCA plot - better to explain analogies:\n",
                "# difference vectors of pickup trucks \"f150\"-\"ford\", \"tacoma\"-\"toyota\" and \n",
                "# \"frontier\"-\"nissan\" are almost parallel. \n",
                "# \"x5\"-\"bmw\" is pointing to a somewhat different direction. \n",
                "\n",
                "model = models['autos_w2v_sg_5'] \n",
                "search = ['ford', 'f150', 'toyota', 'tacoma', 'nissan', 'frontier', 'bmw', 'x5']\n",
                "_ = plot_embeddings(model, search, topn=0, algo='pca', labels=True, colors=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Blueprint: Using Tensorflow Embedding Projector\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "import csv\n",
                "\n",
                "model_path = './models' ###\n",
                "model = models['autos_w2v_sg_30']\n",
                "\n",
                "with open(f'{model_path}/{name}_words.tsv', 'w', encoding='utf-8') as tsvfile:\n",
                "    tsvfile.write('\\n'.join(model.vocab))\n",
                "\n",
                "with open(f'{model_path}/{name}_vecs.tsv', 'w', encoding='utf-8') as tsvfile:\n",
                "    writer = csv.writer(tsvfile, delimiter='\\t', \n",
                "                        dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)\n",
                "    for w in model.vocab:\n",
                "        _ = writer.writerow(model[w].tolist())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "## Blueprint: Constructing a Similarity Tree\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "import networkx as nx\n",
                "from collections import deque\n",
                "\n",
                "def sim_tree(model, word, top_n, max_dist):\n",
                "\n",
                "    graph = nx.Graph()\n",
                "    graph.add_node(word, dist=0)\n",
                "\n",
                "    to_visit = deque([word])\n",
                "    while len(to_visit) > 0:\n",
                "        source = to_visit.popleft() # visit next node\n",
                "        dist = graph.nodes[source]['dist']+1\n",
                "\n",
                "        if dist <= max_dist: # discover new nodes\n",
                "            for target, sim in model.most_similar(source, topn=top_n):\n",
                "                if target not in graph:\n",
                "                    to_visit.append(target)\n",
                "                    graph.add_node(target, dist=dist)\n",
                "                    graph.add_edge(source, target, sim=sim, dist=dist)\n",
                "    return graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "skip"
                }
            },
            "outputs": [],
            "source": [
                "def plt_add_margin(pos, x_factor=0.1, y_factor=0.1):\n",
                "    # rescales the image s.t. all captions fit onto the canvas\n",
                "    x_values, y_values = zip(*pos.values())\n",
                "    x_max = max(x_values)\n",
                "    x_min = min(x_values)\n",
                "    y_max = max(y_values)\n",
                "    y_min = min(y_values)\n",
                "\n",
                "    x_margin = (x_max - x_min) * x_factor\n",
                "    y_margin = (y_max - y_min) * y_factor\n",
                "    # return (x_min - x_margin, x_max + x_margin), (y_min - y_margin, y_max + y_margin)\n",
                "\n",
                "    plt.xlim(x_min - x_margin, x_max + x_margin)\n",
                "    plt.ylim(y_min - y_margin, y_max + y_margin)\n",
                "\n",
                "def scale_weights(graph, minw=1, maxw=8):\n",
                "    # rescale similarity to interval [minw, maxw] for display\n",
                "    sims = [graph[s][t]['sim'] for (s, t) in graph.edges]\n",
                "    min_sim, max_sim = min(sims), max(sims)\n",
                "\n",
                "    for source, target in graph.edges:\n",
                "        sim = graph[source][target]['sim']\n",
                "        graph[source][target]['sim'] = (sim-min_sim)/(max_sim-min_sim)*(maxw-minw)+minw\n",
                "\n",
                "    return graph\n",
                "\n",
                "def solve_graphviz_problems(graph):\n",
                "    # Graphviz has problems with unicode\n",
                "    # this is to prevent errors during positioning\n",
                "    def clean(n):\n",
                "        n = n.replace(',', '')\n",
                "        n = n.encode().decode('ascii', errors='ignore')\n",
                "        n = re.sub(r'[{}\\[\\]]', '-', n)\n",
                "        n = re.sub(r'^\\-', '', n)\n",
                "        return n\n",
                "    \n",
                "    node_map = {n: clean(n) for n in graph.nodes}\n",
                "    # remove empty nodes\n",
                "    for n, m in node_map.items(): \n",
                "        if len(m) == 0:\n",
                "            graph.remove_node(n)\n",
                "    \n",
                "    return nx.relabel_nodes(graph, node_map)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "outputs": [],
            "source": [
                "from networkx.drawing.nx_pydot import graphviz_layout\n",
                "\n",
                "def plot_tree(graph, node_size=1000, font_size=12):\n",
                "    graph = solve_graphviz_problems(graph) ###\n",
                "\n",
                "    pos = graphviz_layout(graph, prog='twopi', root=list(graph.nodes)[0])\n",
                "    plt.figure(figsize=(10, 4), dpi=200) ###\n",
                "    plt.grid(b=None) ### hide box\n",
                "    plt.box(False) ### hide grid\n",
                "    plt_add_margin(pos) ### just for layout\n",
                "\n",
                "    colors = [graph.nodes[n]['dist'] for n in graph] # colorize by distance\n",
                "    nx.draw_networkx_nodes(graph, pos, node_size=node_size, node_color=colors, \n",
                "                           cmap='Set1', alpha=0.4)\n",
                "    nx.draw_networkx_labels(graph, pos, font_size=font_size)\n",
                "    scale_weights(graph) ### not in book\n",
                "    \n",
                "    for (n1, n2, sim) in graph.edges(data='sim'):\n",
                "         nx.draw_networkx_edges(graph, pos, [(n1, n2)], width=sim, alpha=0.2)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "### image may be slightly different to book as models initialized with randoms weights\n",
                "### are not completely comparable\n",
                "model = models['autos_w2v_sg_2']\n",
                "graph = sim_tree(model, 'noise', top_n=10, max_dist=3)\n",
                "plot_tree(graph, node_size=500, font_size=8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false,
                "slideshow": {
                    "slide_type": "subslide"
                }
            },
            "outputs": [],
            "source": [
                "### image may be slightly different to book as models initialized with randoms weights\n",
                "### are not completely comparable\n",
                "model = models['autos_w2v_sg_30']\n",
                "graph = sim_tree(model, 'spark-plug', top_n=8, max_dist=2)\n",
                "plot_tree(graph, node_size=500, font_size=8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Closing Remarks\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Further Reading\n"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {
                "height": "461px",
                "width": "526px"
            },
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "281.575px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
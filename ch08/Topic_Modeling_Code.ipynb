{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/early-release'\n",
    "    os.system(f'wget %{GIT_ROOT}/raw/master/ch08/setup_ch08.py')\n",
    "import setup_ch08 as conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(conf.DEBATES_FILE)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(df.iloc[0][\"text\"][0:200]))\n",
    "print(repr(df.iloc[-1][\"text\"][0:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split('\\.\\s*\\n', text))\n",
    "df[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.groupby('year').agg({'number_of_paragraphs': 'mean'}).plot.bar(figsize=(24,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the paragraphs keeping the years\n",
    "paragraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year } \n",
    "                               for paragraphs, year in zip(df[\"paragraphs\"], df[\"year\"]) \n",
    "                                    for paragraph in paragraphs if paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "tfidf_para_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_para_model = NMF(n_components=10, random_state=42)\n",
    "W_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_para_matrix = nmf_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_para_matrix.sum(axis=0)/W_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_para_model = TruncatedSVD(n_components = 10, random_state=42)\n",
    "W_svd_para_matrix = svd_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_svd_para_matrix = svd_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(svd_para_model, tfidf_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_para_model.singular_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "count_para_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_para_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_para_model, count_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_lda_para_matrix.sum(axis=0)/W_lda_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "lda_display = pyLDAvis.sklearn.prepare(lda_para_model, count_para_vectors, count_para_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_tsne_display = pyLDAvis.sklearn.prepare(lda_para_model, count_para_vectors, count_para_vectorizer, sort_topics=False, mds='tsne')\n",
    "pyLDAvis.display(lda_tsne_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Topic Models and Compare as Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_topics(model, features, no_top_words=40):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        size = {}\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'topic{topic}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(lda_para_model, count_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Development of Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_para_matrix.sum(axis=0)/W_para_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "before_1990 = np.array(paragraph_df[\"year\"] < 1990)\n",
    "after_1990 = ~ before_1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_para_matrix_early = nmf_para_model.transform(tfidf_para_vectors[before_1990])\n",
    "W_para_matrix_late  = nmf_para_model.transform(tfidf_para_vectors[after_1990])\n",
    "print(W_para_matrix_early.sum(axis=0)/W_para_matrix_early.sum()*100.0)\n",
    "print(W_para_matrix_late.sum(axis=0)/W_para_matrix_late.sum()*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['%1.2f'%x for x in W_para_matrix_early.sum(axis=0)/W_para_matrix_early.sum()*100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['%1.2f'%x for x in W_para_matrix_early.sum(axis=0)/W_para_matrix_late.sum()*100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "year_data = []\n",
    "for year in tqdm(np.unique(np.unique(paragraph_df[\"year\"]))):\n",
    "    W_year = nmf_para_model.transform(tfidf_para_vectors[np.array(paragraph_df[\"year\"] == year)])\n",
    "    year_data.append([year] + list(W_year.sum(axis=0)/W_year.sum()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = count_para_vectorizer.get_feature_names()\n",
    "for topic in nmf_para_model.components_:\n",
    "    important = topic.argsort()\n",
    "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "    topic_names.append(\"Topic \" + top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg' \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = pd.DataFrame(year_data, columns=[\"year\"] + topic_names).set_index(\"year\")\n",
    "df_year.plot.area(figsize=(16,9))\n",
    "plt.savefig(\"topic-time.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'Topic {count_para_vectorizer.get_feature_names()[words.argsort()[-1]]}' for words in nmf_para_model.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_para_model.perplexity(tfidf_para_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "lda_para_model_n = []\n",
    "for n in tqdm(range(5, 21)):\n",
    "    lda_model = LatentDirichletAllocation(n_components = n, random_state=42)\n",
    "    lda_model.fit_transform(count_para_vectors)\n",
    "    lda_perplexity = lda_model.perplexity(tfidf_para_vectors)\n",
    "    lda_para_model_n.append((lda_model, lda_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_para_model_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenized documents\n",
    "gensim_paragraphs = [[w for w in re.findall(r'\\b\\w\\w+\\b' , paragraph.lower()) if w not in stopwords] \n",
    "                           for paragraph in paragraph_df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dict_gensim_para = Dictionary(gensim_paragraphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_gensim_para.filter_extremes(no_below=5, no_above=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_gensim_para = [dict_gensim_para.doc2bow(paragraph) for paragraph in gensim_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "tfidf_gensim_para = TfidfModel(bow_gensim_para)\n",
    "vectors_gensim_para = tfidf_gensim_para[bow_gensim_para]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "nmf_gensim_para = Nmf(vectors_gensim_para, num_topics=10, id2word=dict_gensim_para, kappa=0.1, eval_every=5, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_gensim(model):\n",
    "    for topic in range(0, model.num_topics):\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for (word, prob) in model.show_topic(topic, topn=5):\n",
    "            print(\"  %s (%2.2f)\" % (word, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics_gensim(nmf_gensim_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "nmf_gensim_para_coherence = CoherenceModel(model=nmf_gensim_para, texts=gensim_paragraphs, dictionary=dict_gensim_para, coherence='c_v')\n",
    "nmf_gensim_para_coherence_score = nmf_gensim_para_coherence.get_coherence()\n",
    "print(nmf_gensim_para_coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda_gensim_para = LdaModel(corpus=bow_gensim_para, id2word=dict_gensim_para, chunksize=2000,\n",
    "    alpha='auto', eta='auto', iterations=400, num_topics=10, passes=20, eval_every=None, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics_gensim(lda_gensim_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gensim_para.log_perplexity(vectors_gensim_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "lda_gensim_para_coherence = CoherenceModel(model=lda_gensim_para, texts=gensim_paragraphs, dictionary=dict_gensim_para, coherence='c_v')\n",
    "lda_gensim_para_coherence_score = lda_gensim_para_coherence.get_coherence()\n",
    "print(lda_gensim_para_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_gensim_para_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gensim_para.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_gensim_para_coherence = CoherenceModel(model=nmf_gensim_para, texts=gensim_paragraphs, dictionary=dict_gensim_para, coherence='c_v')\n",
    "nmf_gensim_para_coherence_score = nmf_gensim_para_coherence.get_coherence()\n",
    "print(nmf_gensim_para_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = lda_gensim_para.top_topics(vectors_gensim_para, topn=5)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / len(top_topics)\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(t[1], \" \".join([w[1] for w in t[0]])) for t in top_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize number of LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "lda_para_model_n = []\n",
    "for n in tqdm(range(5, 21)):\n",
    "    lda_model = LdaMulticore(corpus=bow_gensim_para, id2word=dict_gensim_para, chunksize=2000,\n",
    "                             eta='auto', iterations=400, num_topics=n, passes=20, \n",
    "                             eval_every=None, random_state=42)\n",
    "    lda_coherence = CoherenceModel(model=lda_model, texts=gensim_paragraphs, \n",
    "                                   dictionary=dict_gensim_para, coherence='c_v')\n",
    "    lda_para_model_n.append((n, lda_model, lda_coherence.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lda_para_model_n, columns=[\"n\", \"model\", \"coherence\"]).set_index(\"n\")[[\"coherence\"]].plot(figsize=(16,9))\n",
    "fig.savefig(\"coherence-topics.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics_gensim(lda_para_model_n[12][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_topics_gensim(model, no_top_words=40):\n",
    "    for topic in range(0, model.num_topics):\n",
    "        size = {}\n",
    "        for (word, prob) in model.show_topic(topic, topn=no_top_words):\n",
    "            size[word] = prob\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'topic{topic}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics_gensim(lda_para_model_n[12][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "hdp_gensim_para = HdpModel(corpus=bow_gensim_para, id2word=dict_gensim_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([1,2], [3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 8\n",
    "pd.DataFrame([re.split(r\" \\+ |\\*\", t[1]) for t in hdp_gensim_para.print_topics(num_topics=20, num_words=words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "pd.DataFrame(hdp_gensim_para.print_topics(num_topics=40, num_words=10), columns=[\"topic\", \"words\"]).set_index(\"topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_gensim_para.show_topic(0, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_gensim_hdp(model, num_topics):\n",
    "    for topic in range(0, num_topics):\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for (word, prob) in model.show_topic(topic, topn=10):\n",
    "            print(\"  %s (%2.2f)\" % (word, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_topics_gensim_hdp(hdp_gensim_para, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means_para = KMeans(n_clusters=10, random_state=42)\n",
    "k_means_para.fit(tfidf_para_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for i in range(10):\n",
    "    sizes.append({\"cluster\": i, \"size\": np.sum(k_means_para.labels_==i)})\n",
    "pd.DataFrame(sizes).set_index(\"cluster\").plot.bar(figsize=(16,9))\n",
    "fig.savefig(\"cluster-size.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(k_means_para.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_clusters(model, vectors, features, no_top_words=40):\n",
    "    for cluster in np.unique(model.labels_):\n",
    "        size = {}\n",
    "        words = vectors[model.labels_ == cluster].sum(axis=0).A[0]\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'cluster{cluster}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_clusters(k_means_para, tfidf_para_vectors, tfidf_para_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap \n",
    "\n",
    "um = umap.UMAP(n_components=2, random_state=0).fit_transform(tfidf_para_vectors)\n",
    "vf = pd.DataFrame(data = um, columns = [\"x\", \"y\"])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(vf.x, vf.y, c=k_means_para.labels_, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TSNE(n_components=2, random_state=0).fit_transform(tfidf_para_vectors)\n",
    "vf = pd.DataFrame(data = um, columns = [\"x\", \"y\"])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(vf.x, vf.y, c=k_means_para.labels_, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = pd.DataFrame(data = ts, columns = [\"x\", \"y\"])\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(vf.x, vf.y, c=k_means_para.labels_, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
